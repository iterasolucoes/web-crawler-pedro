{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import threading\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException        \n",
    "\n",
    "def checking_css_selector(driver, css_selector):\n",
    "    try:\n",
    "        driver.find_element_by_css_selector(css_selector)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "import errno\n",
    "\n",
    "# Creates file path and file if needed\n",
    "def creating_files(file_path, csv_file, csv_columns):\n",
    "    # Checking if file path exist, if not, create it\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            os.makedirs(file_path)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "    # else:\n",
    "    #     print(\"File path already exists\")\n",
    "    # Creating file and columns variables of the csv file\n",
    "    if os.path.isfile(os.path.join(file_path, csv_file)) is False:\n",
    "        try:\n",
    "            with open(os.path.join(file_path, csv_file), 'w', encoding='utf-8', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "                writer.writeheader()\n",
    "                csvfile.close()\n",
    "        except IOError:\n",
    "            print(\"I/O error\")\n",
    "    else:\n",
    "        print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_row(file, csv_columns, data_list):\n",
    "    try:\n",
    "        with open(file, 'a', encoding='utf-8', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "            for doctor in data_list:\n",
    "                dict_data = {\n",
    "                            csv_columns[0]: doctor[\"name\"],\n",
    "                            csv_columns[1]: doctor[\"specialties\"],\n",
    "                            csv_columns[2]: doctor[\"skills\"],\n",
    "                            csv_columns[3]: doctor[\"state\"],\n",
    "                            csv_columns[4]: doctor[\"city\"],\n",
    "                            csv_columns[5]: doctor[\"phone\"],\n",
    "                }\n",
    "                writer.writerow(dict_data)\n",
    "            csvfile.close()\n",
    "    except IOError:\n",
    "        print(\"I/O error: \", sys.exc_info())\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_data(driver, path, csv_columns):\n",
    "    #  creating data_list\n",
    "    data_list = []\n",
    "    #  creatind data dictionray. Pandas could be used here. Nevertheless I prefer to create the data structure\n",
    "    data_dict = {'name': '', 'specialties': '', 'skills': '', 'state': '', 'city': '', 'phone': ''}\n",
    "    doctors_list = []\n",
    "    url = driver.current_url\n",
    "    soup_level2 = BeautifulSoup(driver.page_source)\n",
    "    \n",
    "    try:\n",
    "        #  getting all doctor urls from page\n",
    "        for link in soup_level2.findAll('a', class_=\"rank-element-name__link\"):\n",
    "            doctors_list.append(link.get('href'))\n",
    "        \n",
    "        #  crawling data for each doctor \n",
    "        for doctor in doctors_list:\n",
    "            driver.get(doctor)\n",
    "            soup_level3 = BeautifulSoup(driver.page_source)\n",
    "            \n",
    "            #  crawling doctor name\n",
    "            if soup_level3.find('div', class_=\"unified-doctor-header-info__name\"):\n",
    "                if (soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText() == \n",
    "                    soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span', itemprop=\"name\").getText()):\n",
    "                    data_dict['name'] = soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText()\n",
    "                else:\n",
    "                    data_dict['name'] = soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText() + ' ' + soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span', itemprop=\"name\").getText()\n",
    "            \n",
    "            #  crawling doctor specialties\n",
    "            temp_specialties = []\n",
    "            if soup_level3.find(class_=\"h4 text-muted text-base-weight offset-bottom-0\"):\n",
    "                h2 = soup_level3.find('h2', class_='h4 text-muted text-base-weight offset-bottom-0')\n",
    "                for specialties  in h2.findAll('a', class_= \"text-muted\"):\n",
    "                    temp_specialties.append(specialties['title'])\n",
    "                if len(temp_specialties) != 0:\n",
    "                    data_dict['specialties'] = temp_specialties\n",
    "                else:\n",
    "                    temp_specialties.append(h2.find('span').getText())\n",
    "                    data_dict['specialties'] = temp_specialties\n",
    "            del temp_specialties\n",
    "\n",
    "            #  crawling city and state and removing duplicants\n",
    "            temp_city =[]\n",
    "            temp_state =[]\n",
    "            if soup_level3.findAll('h5', class_=\"offset-0\"):\n",
    "                for h5 in soup_level3.findAll('h5', class_=\"offset-0\"):\n",
    "                    for city in h5.findAll('span', class_=\"city\"):\n",
    "                        if city['content'] not in temp_city:\n",
    "                            temp_city.append(city['content'])\n",
    "                    for state in h5.findAll('span', class_='province region'):\n",
    "                        if state['content'][-2:] not in temp_state:\n",
    "                            temp_state.append(state['content'][-2:])\n",
    "                if len(temp_city) != 0:\n",
    "                    data_dict['city'] = temp_city\n",
    "                if len(temp_state) != 0:\n",
    "                    data_dict['state'] = temp_state\n",
    "            del temp_city\n",
    "            del temp_state\n",
    "            \n",
    "            #  crawling skills\n",
    "            temp_skills = []\n",
    "            if soup_level3.findAll('p', class_=\"offset-bottom-0 offset-right-1\"):\n",
    "                for link in soup_level3.findAll('p', class_=\"offset-bottom-0 offset-right-1\"):\n",
    "                    temp_skills.append(link.getText().strip())\n",
    "                data_dict['skills'] = temp_skills\n",
    "            del temp_skills\n",
    "            \n",
    "            #  crawling phones and removing duplicants\n",
    "            temp_phone = []\n",
    "            if soup_level3.findAll('a', class_= \"text-muted padding-left-2\"):\n",
    "                for phone in soup_level3.findAll('a', class_= \"text-muted padding-left-2\"):\n",
    "                    if phone['href'][4:] not in temp_phone:\n",
    "                        temp_phone.append(phone['href'][4:])\n",
    "                data_dict['phone'] = temp_phone\n",
    "            del temp_phone\n",
    "            \n",
    "            data_list.append(deepcopy(data_dict))\n",
    "            \n",
    "        #  cleaning data\n",
    "        del data_dict\n",
    "        del doctors_list          \n",
    "        \n",
    "        write_data_row(path, csv_columns, data_list)\n",
    "        del data_list\n",
    "        \n",
    "        #  moving to next page recursively\n",
    "        driver.get(url)\n",
    "        del url\n",
    "        while(checking_css_selector(driver, '.next')):\n",
    "            driver.get(soup_level2.find('li', class_=\"next\").find('a')['href'])\n",
    "            crawling_data(driver, path, csv_columns)   \n",
    "    \n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        raise\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling function\n",
    "def crawling_specialties(url, path, csv_columns):\n",
    "    #  create a new Firefox session\n",
    "    #  driver = webdriver.Firefox(executable_path = '/geckodriver')\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.implicitly_wait(30)\n",
    "    driver.get(url)\n",
    "    crawling_data(driver, path, csv_columns)\n",
    "    driver.close()\n",
    "    del driver\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  threaded function for queue processing.\n",
    "def crawl(q, csv_columns):\n",
    "    while not q.empty():\n",
    "        tp = q.get()  #  fetch new work from the Queue\n",
    "        try:\n",
    "            driver = webdriver.Firefox()\n",
    "            driver.implicitly_wait(30)\n",
    "            driver.get(tp[1])  #  url\n",
    "            crawling_data(driver, tp[0], csv_columns)  # tp[0] correspond to te file path\n",
    "            driver.close()\n",
    "        except:\n",
    "            #logging.error('Error with URL check!')\n",
    "            print('Error with URL check!: '+ str(tp[1]))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  launch global variables\n",
    "url = \"https://www.doctoralia.com.br\"\n",
    "main_page = \"/especializacoes-medicas\"\n",
    "#geckodriver_path = \"geckodriver\"\n",
    "\n",
    "#  subtree of medical_specialties\n",
    "medical_specialties = []\n",
    "\n",
    "#  csv variables\n",
    "csv_path = \"csv_files\"\n",
    "csv_columns = ['Nome',\n",
    "               'Especialidade(s)',\n",
    "               'CompetÃªncia(s)',\n",
    "               'Estado',\n",
    "               'Cidade',\n",
    "               'Telefone'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a new Firefox session\n",
    "#  driver = webdriver.Firefox(executable_path = '/geckodriver')\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.get(url+main_page)\n",
    "\n",
    "#  getting all medical specialties\n",
    "soup_level1 = BeautifulSoup(driver.page_source)\n",
    "for link in soup_level1.findAll('a', class_=\"text-muted\"):\n",
    "    if link.get('href').count('/') == 1:\n",
    "        medical_specialties.append(link.get('href'))\n",
    "#  medical_specialties\n",
    "driver.close()\n",
    "del driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for specialtie in medical_specialties:\n",
    "    creating_files(csv_path, specialtie[1:] + \".csv\", csv_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  threads\n",
    "num_threads = min(5, len(medical_specialties))\n",
    "q = queue.Queue(maxsize=0)\n",
    "\n",
    "for i in range(len(medical_specialties)):\n",
    "    #need the index and the url in each queue item.\n",
    "    q.put((os.path.join(csv_path, medical_specialties[i][1:] + \".csv\"), url+medical_specialties[i]))\n",
    "\n",
    "#for i in range(num_threads):\n",
    "#    tp = q.get(i)\n",
    "#    print(tp[0])\n",
    "#    print(tp[1])\n",
    "#    print(type(q.get(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load up the queue with the urls to fetch and the index for each job (as a tuple):\n",
    "for i in range(num_threads):\n",
    "    #logging.debug('Starting thread ', i)\n",
    "    print('Starting thread ', i)\n",
    "    #print('   ' + str(url+medical_specialties[1]))\n",
    "    worker = threading.Thread(target=crawl, args=(q, csv_columns))\n",
    "    #worker.setDaemon(True)     \n",
    "    worker.start()\n",
    "q.join()\n",
    "    #  need the index and the url in each queue item.\n",
    "    #q.put((i,medical_specialties[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = threading.Thread(target=crawling_specialties, args=(url+medical_specialties[0], \n",
    "                                                         os.path.join(csv_path, medical_specialties[0][1:] + \".csv\"), \n",
    "                                                         csv_columns))\n",
    "#  append thread to list\n",
    "list_thread.append(x1)\n",
    "#  start thread\n",
    "x1.start()\n",
    "\n",
    "x2 = threading.Thread(target=crawling_specialties, args=(url+medical_specialties[1], \n",
    "                                                         os.path.join(csv_path, medical_specialties[1][1:] + \".csv\"), \n",
    "                                                         csv_columns))\n",
    "#  append thread to list\n",
    "list_thread.append(x2)\n",
    "#  start thread\n",
    "x2.start()\n",
    "\n",
    "\n",
    "#  wait until threads finish their job\n",
    "for th in list_thread:\n",
    "    th.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(medical_specialties):\n",
    "    x = threading.Thread(target=crawling_specialties, args=(url+medical_specialties[idx], \n",
    "                                                            os.path.join(csv_path, item[1:] + \".csv\"), \n",
    "                                                            csv_columns))\n",
    "    #  setting a thread name for debug\n",
    "    x.setName('Thread name: ' + item + '. Thread number: ' + str(idx))\n",
    "    #  append thread to list\n",
    "    list_thread.append(x)\n",
    "    #  start thread\n",
    "    x.start()\n",
    "\n",
    "#  wait until threads finish their job\n",
    "for th in list_thread:\n",
    "    th.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup_level3 = BeautifulSoup(driver.page_source)\n",
    "\n",
    "if soup_level3.find(class_=\"unified-doctor-header-info__name\"):\n",
    "        if (soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText() == \n",
    "            soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span', itemprop=\"name\").getText()):\n",
    "            print(soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText())\n",
    "        else:\n",
    "            print(soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText() + ' ' + soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span', itemprop=\"name\").getText())\n",
    "            \n",
    "\n",
    "print('---')\n",
    "\n",
    "h2 = soup_level3.find('h2', class_='h4 text-muted text-base-weight offset-bottom-0')\n",
    "print(h2)\n",
    "for specialties in h2.findAll('a', class_= \"text-muted\"):\n",
    "        print(specialties['title'])\n",
    "\n",
    "print('---')\n",
    "temp_city =[]\n",
    "temp_state =[]\n",
    "\n",
    "#div = soup_level3.find('div', class_= 'panel-body')\n",
    "#div = soup_level3.find(lambda tag: tag.name == 'div' and tag['class'] == ['panel-body'])\n",
    "div = soup_level3.find(lambda tag: tag.name == 'div' and tag.get('class') == ['panel-body'])\n",
    "for h5 in soup_level3.findAll('h5', class_=\"offset-0\"):\n",
    "    for city in h5.findAll('span', class_=\"city\"):\n",
    "        print(city['content'])\n",
    "    for state in h5.findAll('span', class_='province region'):\n",
    "        print(state['content'][-2:])\n",
    "#for link in soup_level3.findAll('span', class_=\"province region\"):\n",
    "#    if link['content'][:-3] not in temp_city:\n",
    "#        temp_city.append(link['content'][:-3])\n",
    "#    if link['content'][-2:] not in temp_state:\n",
    "#        temp_state.append(link['content'][-2:])\n",
    "#print(temp_city, temp_state)\n",
    "print('---')\n",
    "del temp_city\n",
    "del temp_state\n",
    "temp_skills = []\n",
    "for link in soup_level3.findAll('p', class_=\"offset-bottom-0 offset-right-1\"):\n",
    "    temp_skills.append(link.getText().strip())\n",
    "print(temp_skills)\n",
    "print('---')\n",
    "del temp_skills\n",
    "temp_phone = []\n",
    "for phone in soup_level3.findAll('a', class_= \"text-muted padding-left-2\"):\n",
    "    if phone['href'][4:] not in temp_phone:\n",
    "        temp_phone.append(phone['href'][4:])\n",
    "print(temp_phone)\n",
    "del temp_phone\n",
    "\n",
    "#results = r.findall(soup_level3.find('div', class_= \"modal fade\").getText().strip())\n",
    "#for x in results:\n",
    "#        print(x)\n",
    "\n",
    "        #print(soup_level3.find('div', class_= \"modal fade\").getText().strip())\n",
    "#print(soup_level3.findAll('button', class_=\"btn btn-sm btn-default\"))\n",
    "#print(soup_level3.findAll('a', class_= \"text-muted padding-left-2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "data_dict = {'name': '', 'specialties': '', 'skills': '', 'state': '', 'city': '', 'phone': ''}\n",
    "    \n",
    "try:\n",
    "    soup_level3 = BeautifulSoup(driver.page_source)\n",
    "    #  crawling doctor name\n",
    "    if soup_level3.find(class_=\"unified-doctor-header-info__name\"):\n",
    "        if (soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText() == \n",
    "            soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span', itemprop=\"name\").getText()):\n",
    "            data_dict['name'] = soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText()\n",
    "        else:\n",
    "            data_dict['name'] = soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText() + ' ' + soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span', itemprop=\"name\").getText()\n",
    "            \n",
    "    #  crawling doctor specialties\n",
    "    temp_specialties = []\n",
    "    if soup_level3.find('h2', class_=\"h4 text-muted text-base-weight offset-bottom-0\"):\n",
    "        h2 = soup_level3.find('h2', class_='h4 text-muted text-base-weight offset-bottom-0')\n",
    "        for specialties  in h2.findAll('a', class_= \"text-muted\"):\n",
    "            temp_specialties.append(specialties['title'])\n",
    "        if len(temp_specialties) != 0:\n",
    "            data_dict['specialties'] = temp_specialties\n",
    "        else:\n",
    "            temp_specialties.append(h2.find('span').getText())\n",
    "            data_dict['specialties'] = temp_specialties\n",
    "    del temp_specialties\n",
    "\n",
    "    #  crawling city and state and removing duplicants\n",
    "    temp_city =[]\n",
    "    temp_state =[]\n",
    "    if soup_level3.findAll('h5', class_=\"offset-0\"):\n",
    "        for h5 in soup_level3.findAll('h5', class_=\"offset-0\"):\n",
    "            for city in h5.findAll('span', class_=\"city\"):\n",
    "                if city['content'] not in temp_city:\n",
    "                    temp_city.append(city['content'])\n",
    "            for state in h5.findAll('span', class_='province region'):\n",
    "                if state['content'][-2:] not in temp_state:\n",
    "                    temp_state.append(state['content'][-2:])\n",
    "        data_dict['city'] = temp_city\n",
    "        data_dict['state'] = temp_state\n",
    "    del temp_city\n",
    "    del temp_state\n",
    "   \n",
    "    #  crawling skills\n",
    "    temp_skills = []\n",
    "    if soup_level3.findAll('p', class_=\"offset-bottom-0 offset-right-1\"):\n",
    "        for link in soup_level3.findAll('p', class_=\"offset-bottom-0 offset-right-1\"):\n",
    "            temp_skills.append(link.getText().strip())\n",
    "        data_dict['skills'] = temp_skills\n",
    "    del temp_skills\n",
    "            \n",
    "    #  crawling phones and removing duplicants\n",
    "    temp_phone = []\n",
    "    if soup_level3.findAll('a', class_= \"text-muted padding-left-2\"):\n",
    "        for phone in soup_level3.findAll('a', class_= \"text-muted padding-left-2\"):\n",
    "            if phone['href'][4:] not in temp_phone:\n",
    "                temp_phone.append(phone['href'][4:])\n",
    "        data_dict['phone'] = temp_phone\n",
    "    del temp_phone\n",
    "            \n",
    "    data_list.append(deepcopy(data_dict))\n",
    "            \n",
    "    #  cleaning data\n",
    "    del data_dict  \n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    raise\n",
    "\n",
    "for data in data_list:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_data(driver, data_list = None):\n",
    "    #  creating data_list if doesnt exist\n",
    "    if data_list is None:\n",
    "        data_list = []\n",
    "    #  creatind data dictionray. Pandas could be used here. Nevertheless I prefer to create the data structure\n",
    "    data_dict = {'name': '', 'specialties': '', 'skills': '', 'state': '', 'city': '', 'phone': ''}\n",
    "    doctors_list = []\n",
    "    url = driver.current_url\n",
    "    soup_level2 = BeautifulSoup(driver.page_source)\n",
    "    \n",
    "    try:\n",
    "        #  getting all doctor urls from page\n",
    "        for link in soup_level2.findAll('a', class_=\"rank-element-name__link\"):\n",
    "            doctors_list.append(link.get('href'))\n",
    "        \n",
    "        #  crawling data for each doctor \n",
    "        for doctor in doctors_list:\n",
    "            driver.get(doctor)\n",
    "            soup_level3 = BeautifulSoup(driver.page_source)\n",
    "            \n",
    "            #  crawling doctor name\n",
    "            if soup_level3.find('div', class_=\"unified-doctor-header-info__name\"):\n",
    "                if (soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText() == \n",
    "                    soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span', itemprop=\"name\").getText()):\n",
    "                    data_dict['name'] = soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText()\n",
    "                else:\n",
    "                    data_dict['name'] = soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span').getText() + ' ' + soup_level3.find('div', class_=\"unified-doctor-header-info__name\").find('span', itemprop=\"name\").getText()\n",
    "            \n",
    "            #  crawling doctor specialties\n",
    "            temp_specialties = []\n",
    "            if soup_level3.find(class_=\"h4 text-muted text-base-weight offset-bottom-0\"):\n",
    "                h2 = soup_level3.find('h2', class_='h4 text-muted text-base-weight offset-bottom-0')\n",
    "                for specialties  in h2.findAll('a', class_= \"text-muted\"):\n",
    "                    temp_specialties.append(specialties['title'])\n",
    "                if len(temp_specialties) != 0:\n",
    "                    data_dict['specialties'] = temp_specialties\n",
    "                else:\n",
    "                    temp_specialties.append(h2.find('span').getText())\n",
    "                    data_dict['specialties'] = temp_specialties\n",
    "            del temp_specialties\n",
    "\n",
    "            #  crawling city and state and removing duplicants\n",
    "            temp_city =[]\n",
    "            temp_state =[]\n",
    "            if soup_level3.findAll('h5', class_=\"offset-0\"):\n",
    "                for h5 in soup_level3.findAll('h5', class_=\"offset-0\"):\n",
    "                    for city in h5.findAll('span', class_=\"city\"):\n",
    "                        if city['content'] not in temp_city:\n",
    "                            temp_city.append(city['content'])\n",
    "                    for state in h5.findAll('span', class_='province region'):\n",
    "                        if state['content'][-2:] not in temp_state:\n",
    "                            temp_state.append(state['content'][-2:])\n",
    "                if len(temp_city) != 0:\n",
    "                    data_dict['city'] = temp_city\n",
    "                if len(temp_state) != 0:\n",
    "                    data_dict['state'] = temp_state\n",
    "            del temp_city\n",
    "            del temp_state\n",
    "            \n",
    "            #  crawling skills\n",
    "            temp_skills = []\n",
    "            if soup_level3.findAll('p', class_=\"offset-bottom-0 offset-right-1\"):\n",
    "                for link in soup_level3.findAll('p', class_=\"offset-bottom-0 offset-right-1\"):\n",
    "                    temp_skills.append(link.getText().strip())\n",
    "                data_dict['skills'] = temp_skills\n",
    "            del temp_skills\n",
    "            \n",
    "            #  crawling phones and removing duplicants\n",
    "            temp_phone = []\n",
    "            if soup_level3.findAll('a', class_= \"text-muted padding-left-2\"):\n",
    "                for phone in soup_level3.findAll('a', class_= \"text-muted padding-left-2\"):\n",
    "                    if phone['href'][4:] not in temp_phone:\n",
    "                        temp_phone.append(phone['href'][4:])\n",
    "                data_dict['phone'] = temp_phone\n",
    "            del temp_phone\n",
    "            \n",
    "            data_list.append(deepcopy(data_dict))\n",
    "            \n",
    "        #  cleaning data\n",
    "        del data_dict\n",
    "        del doctors_list\n",
    "        \n",
    "        #  moving to next page recursively\n",
    "        print(\"The length of list is: \", len(data_list)) \n",
    "        driver.get(url)\n",
    "        del url\n",
    "        while(checking_css_selector(driver, '.next')):\n",
    "            driver.get(soup_level2.find('li', class_=\"next\").find('a')['href'])\n",
    "            crawling_data(driver, data_list)   \n",
    "    \n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        raise\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threading function\n",
    "def crawling_specialties(url, path, csv_columns):\n",
    "    #  create a new Firefox session\n",
    "    #  driver = webdriver.Firefox(executable_path = '/geckodriver')\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.implicitly_wait(30)\n",
    "    driver.get(url)\n",
    "    data_list = crawling_data(driver)\n",
    "    write_data_row(path, csv_columns, data_list)\n",
    "    del data_list\n",
    "    driver.close()\n",
    "    return True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
